{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.SJob \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ficheJob']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.list_collection_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprés voir insérer manuellement un document dans le sell de mongodb, nous pouvons le visualiser avec pprint.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5fad3437891d89e98649fc80'),\n",
      " 'description': 'FL-Tampa, Our client, one of the largest financial services '\n",
      "                'firms, is seeking a Big Data Developer Location: Tampa, FL '\n",
      "                'Position Type: Contract Job Responsibilities: -Gather various '\n",
      "                'data points from multiple sources within and load to big data '\n",
      "                'Hadoop solution. -Transform data into usable form per user '\n",
      "                'requirements. -Implement proper controls to protect integrity '\n",
      "                'of data and limit use of sensitive data to una',\n",
      " 'guid': '221978898',\n",
      " 'link': 'http://jobview.monster.com/Big-Data-Developer-Job-Tampa-FL-US-221978898.aspx',\n",
      " 'pubDate': 'Thu, 12 Nov 2020 02:31:23 GMT',\n",
      " 'title': 'Big Data Developer'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(db.ficheJob.find_one())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insérer un document avec les caractérisques suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "FJob = { \"title\" : \"Big Data Developer - II\",\n",
    "        \"description\" : \"NY-Garden City, Location: Garden City, NY Description: Our client is currently seeking a Big Data Developer - II Title: Big Data Developer Location: Garden City, NY Duration: 12+ Months What you?ll be doing. You will be part of a new Data Science team, ensuring that innovation drives continuous business improvements, bringing more profitable growth and an even better customer experience. As a Data Analytics/Predi\",\n",
    "\n",
    "        \"link\": \"http://jobview.monster.com/Big-Data-Developer-II-Job-Garden-City-NY-US-221152770.aspx\",\n",
    "        \"guid\":\"221152770\",\n",
    "        \"pubDate\": \"Thu, 12 Nov 2020 00:14:37 GMT\"\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiches = db.ficheJob\n",
    "FjJob_id = fiches.insert_one(FJob).inserted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5fb5349c9e1c992033b5454e'),\n",
       " 'title': 'Big Data Developer - II',\n",
       " 'description': 'NY-Garden City, Location: Garden City, NY Description: Our client is currently seeking a Big Data Developer - II Title: Big Data Developer Location: Garden City, NY Duration: 12+ Months What you?ll be doing. You will be part of a new Data Science team, ensuring that innovation drives continuous business improvements, bringing more profitable growth and an even better customer experience. As a Data Analytics/Predi',\n",
       " 'link': 'http://jobview.monster.com/Big-Data-Developer-II-Job-Garden-City-NY-US-221152770.aspx',\n",
       " 'guid': '221152770',\n",
       " 'pubDate': 'Thu, 12 Nov 2020 00:14:37 GMT'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiches.find_one({'title': 'Big Data Developer - II'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sacia/anaconda3/envs/MachineLearning/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compter le nombre d'éléments ds la base \n",
    "db.ficheJob.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# permet de diriger le résultat du cral ds un fichier json\n",
    "scrapy crawl flashbot -o posts.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dTitle of document inserted ['Machine Learning Engineer - Remote Opportunity (PST/MST)']\n",
      "dTitle of document inserted ['Machine Learning Engineer, Computer Vision']\n",
      "dTitle of document inserted ['Machine Learning Engineer - Computer Vision']\n",
      "dTitle of document inserted ['Mid-Level Machine Learning Engineer - Facial Recognition']\n",
      "dTitle of document inserted ['Machine Learning Researcher- (Generative Adversarial Networks)']\n",
      "dTitle of document inserted ['Senior Machine Learning Engineer']\n",
      "dTitle of document inserted ['MACHINE LEARNING ENGINEER']\n",
      "dTitle of document inserted ['Principal Machine Learning Scientist']\n",
      "dTitle of document inserted ['Machine Learning -Intelligent Mobility System Engineer (AVARD898824)']\n",
      "dTitle of document inserted ['Machine Learning Engineer']\n",
      "dTitle of document inserted ['Machine operator']\n",
      "dTitle of document inserted ['Machine Operator - Flavoring 1st Shift']\n",
      "dTitle of document inserted ['Machine Operator - Flavoring 2nd Shift']\n",
      "dTitle of document inserted ['Machine Assembly Intern']\n",
      "dTitle of document inserted ['Machine Vision Software Specialist']\n",
      "dTitle of document inserted ['Packaging Machine Operator']\n",
      "dTitle of document inserted ['Machine Operator']\n",
      "dTitle of document inserted ['3rd shift machine operator']\n",
      "dTitle of document inserted ['Machine Operator']\n",
      "dTitle of document inserted ['EDM Machine Operator']\n",
      "dTitle of document inserted ['R&D Machine Analyst']\n",
      "dTitle of document inserted ['Deep Learning Engineer - Display Algorithms']\n",
      "dTitle of document inserted ['Principal/Lead Research Engineer - Reinforcement Learning']\n",
      "dTitle of document inserted ['Scientist - Disease Modeling - Mammalian Cell Culture, FACS']\n",
      "dTitle of document inserted ['Associate Scientist, Liver Phenotypic Assays']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sacia/anaconda3/envs/MachineLearning/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "with open(\"/home/sacia/BriefScrapy/flashbotN/flashbotN/posts.json\") as f:\n",
    "   file_data = json.load(f) \n",
    "\n",
    "for ident in file_data:\n",
    "    guid = ident [\"guid\"] [0]\n",
    "    \n",
    "    rest= db.ficheJob.find ({\"guid\":guid})\n",
    "    \n",
    "    if rest.count() == 0:\n",
    "       db.ficheJob.insert_one(ident)\n",
    "       print(\"dTitle of document inserted\", ident[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Collection.count of Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'SJob'), 'ficheJob')>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.ficheJob.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sacia/anaconda3/envs/MachineLearning/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.ficheJob.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Collection' object is not callable. If you meant to call the 'createIndex' method on a 'Collection' object it is failing because no such method exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-77d0e12fadd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Créer un index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mficheJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateIndex\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m\"title\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"description\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"text\"\u001b[0m \u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.7/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3446\u001b[0m                         \u001b[0;34m\"call the '%s' method on a 'Collection' object it is \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3447\u001b[0m                         \u001b[0;34m\"failing because no such method exists.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3448\u001b[0;31m                         self.__name.split(\".\")[-1])\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Collection' object is not callable. If you meant to call the 'createIndex' method on a 'Collection' object it is failing because no such method exists."
     ]
    }
   ],
   "source": [
    "# Créer un index dans un shell mongodb\n",
    "\n",
    "db.ficheJob.createIndex( { \"title\" : \"text\", \"description\": \"text\" } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-55-5c3a8b1f06a0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-55-5c3a8b1f06a0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    db.ficheJob.find( { $text: { $search: \"Data\" } } )\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "db.ficheJob.find( { $text: { $search: \"Data\" } } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ds le fichier settings.py, il faudrait désavctiver cette ligne afin d'indiquer à Scrapy d'envoyer le résultat du crawl dans \"item\", qui sera récupérer dans le fihier piplines.py.\n",
    "ce dernier se chargera par la suite d'inserer les données dans la base mongodb.\n",
    "\n",
    "\n",
    "ITEM_PIPELINES = {\n",
    "   'flashbotN.pipelines.FlashbotnPipeline': 300,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
